---
title: "[Developing] Basic Statistics Review for Data Science"
date: 2019-10-13
tags: [Python, Statistics]
header:
  image: "/images/rally/title.png"
  teaser: "/images/animation/time_series.gif"
excerpt: "This is a sample tutorial module on basic statistics for data science applications."
mathjax: true

---
<div id="fb-root"></div>
<script async defer src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2"></script>

This is a draft for a basic statistics module for data scientists. It assumes that the reader knows some basic Python.

---

<b> Outline </b>
+ Measures of Central Tendencies
+ Measures of Dispersion
+ Correlation and Causation
+ Probability
    - Dependence and Independence
    - Conditional Probability
    - Bayes' Theorem
    - Random Variables
    - Continuous distributions
    - Normal distribution
    - Central Limit Theorem
+ Hypothesis and interference
    - Confidence interval
    - Bayesian inference

In the recent decade, we have produced the majority of our data due to increased in computing powers of our machines, increase in volume of our storage systems and the decrease in the cost of technology making it accessible to a lot more people.

But data is just that - data.




Let us start by generating random data. Assuming we want to observe the variation in temperature patterns across time. Using **numpy** and **random** library in Python:



Take for example the qualifying exam for **Eskwelabs**. Assuming that there are 100 items and 500 examinees. You have already learned by now how to generate synthetic data in Python. We generate the scores randomly and visualize using a histogram with the hope that no one scores below 50.

Eskwelabs had 500 applicants. They tried to have an exam.

We plot in python. Visualizing can give us a feel of what the data looks like. We can also find the minimum and maximum values number of examinees and range of scores.

Most of the time, visualizing data is not enough to get insights.



Statistics is not only inseparable with data science, **it is one of the core**. In simple terms, it refers to mathematical methods and techniques to gain insights from data. How do we make statistics lesson more fun? huhuhu

One of the core of statistics is the:

<h2>**Measures of Central Tendencies**</h2>

In exploratory data analysis, we want to check the average or central values of our data. Centrality can be measured by these 3 values: mean, median, and mode.

mean <br>

The mean is the most common metric used. (list advantages and disadvantages, sample code and sample problem) it gives an initial glance of a distribution. This is just the sum of the individual data points divided the total number of data points.

Essentially, this equation gives the mean:

$$
\begin{equation}
\overline x = \sum_{i}^{N} \frac {x_i}{N}
\end{equation}
$$

where $$x_i$$ is the individual data points and N is the total number of points.

In python, we can define a function to calculate the mean as:

<script src="https://gist.github.com/albertyumol/a532da3d311d913247111c2485096231.js"></script>

Another measure of central tendency is the

**median**

which is the middle value when you sort out the data points from highest to lowest. We consider to cases when calculating the median of a data set. The first is when the number of data points is odd. This means that this number is not divisible by **two**, meaning it has a unique middle value. This unique middle value is the median. The other case is when the number of the data points is even, implying that the number of data points is divisible by two. There are two candidates for the median. What we do is we get the average of these points. Here is how we do it in Python.

Essentially, this equation gives the median:

for odd case:

$$
\begin{equation}
M_{odd} = \left( \frac {N+1}{N} \right) ^{th} term
\end{equation}
$$

for even case:

$$
\begin{equation}
M_{even} = \frac {\left( \frac {N}{2} \right) ^{th} term + \left( \frac {N}{2} + 1 \right) ^{th} term}{2}
\end{equation}
$$

Implementing it in Python, 

<script src="https://gist.github.com/albertyumol/15011622fbd3747ae541f45b5814b002.js"></script>

take note that middle value is minus one because Python starts counting from 0.


As we can see median is a bit more complex on the computation side compared to the mean that's why the latter is used more frequently. There are other ways to calculate the median without sorting. Check this out:

[Sorting Less Median Calculation](https://medium.com/@nxtchg/calculating-median-without-sorting-eaa639cedb9f)

The problem with the mean is that it is very sensitive to outliers unlike the median. So if your analysis requires sensitivity accounting to outliers the median is the better option.

To do a little of side track, median is part of a more generalized data descriptors called the quantiles. Basically quantiles just divide the data set in equal proportions. The median 2-quantile is the middle value when the data set is divided into two equal proportions.





mode <br>





(Assuming as well that you know some basic Python.)

~~~Python
import random
import pandas as pd
import matplotlib.pyplot as plt

random.seed( 30 )
n = 12
y = []
for i in range(n):
    y += [random.randint(15, 30)]

z = pd.DataFrame(y)
z[1] = z.index + 1
z = z[[1,0]]
z.columns = ['Month', 'Temperature']
plt.bar(z['Month'], z['Temperature'])
~~~

***change lables to actial months and polish the storyline***

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/rally/social_movement2.gif" alt="Philippine rallies." class="center">
{: refdef}

**insert code here**






Develop a storyline.


Statistics has two main branch, univariate and multivariate.

For this module, our focus is on univariate statistics.

Testing if my math equations will load

$$\nabla_\boldsymbol{x} J(\boldsymbol{x})$$

$$R_0$$



Testing another equation

$$
\begin{equation}
SE(\hat{p_1}-\hat{p_2}) = \sqrt{\frac{\hat{p}_{combined}(1-\hat{p}_{combined})}{n_1}+\frac{\hat{p}_{combined}(1-\hat{p}_{combined})}{n_2}}
\end{equation}
$$



SCRATCH:
Editing stat article ().

This is a guide on Markdown [Stat][1].

we generated majority of our data. We have become walking data sets with the technologies that we use constantly emitting metadata.

Not only around us but we also generate a lot of data. Through our mobile phones and wearable tech such as smart watches and other smarts whatsoever.



Here are my resources:

**References**

[1]: http://en.wikipedia.org/wiki/Markdown        "Stat"

[1] Joel Grus. Data Science from Scratch: First Principles with Python.

[2] something. something. Retrieved from:
[something](https://en.wikipedia.org/wiki/Global_Database_of_Events,_Language,_and_Tone)

[3] something. Retrieved from: [something](https://www.gdeltproject.org/)

[4] something. something. Retrieved from: [something](https://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations)

[5] something. something. Retrieved from: [something](https://www.hindawi.com/journals/ddns/2017/8180272/)

[6] something. something. Retrieved from: [https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-019-0183-y](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-019-0183-y)

[7] something. something. Retrieved from :
[something](https://tenor.com/view/well-be-watching-you-greta-thunberg-gif-15167876)


<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6410209740119334",
    enable_page_level_ads: true
  });
</script>

<div class="fb-comments" data-href="https://albertyumol.github.io/" data-numposts="5"></div>
