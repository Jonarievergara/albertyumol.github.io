---
title: "[Developing] Basic Statistics Review for Data Science"
date: 2019-10-13
tags: [Python, Statistics]
header:
  image: "/images/rally/title.png"
  teaser: "/images/animation/time_series.gif"
excerpt: "This is a sample tutorial module on basic statistics for data science applications."
mathjax: true

---
<div id="fb-root"></div>
<script async defer src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2"></script>

This is a draft for a basic statistics module for data scientists. It assumes that the reader knows some basic Python.

---

<b> Outline </b>
+ Measures of Central Tendencies
+ Measures of Dispersion
+ Covariance, Correlation, and Causation
+ Probability
    - Dependence and Independence
    - Conditional Probability
    - Bayes' Theorem
    - Random Variables
    - Continuous distributions
    - Normal distribution
    - Central Limit Theorem
+ Hypothesis and interference
    - Confidence interval
    - Bayesian inference

In the recent decade, we have produced the majority of our data due to increased in computing powers of our machines, increase in volume of our storage systems and the decrease in the cost of technology making it accessible to a lot more people.

But data is just that - data. For us to get insights, we need to process.

Take as an example the qualifying entrance exam for **Eskwelabs**. Assume that there are 50 items for the exam with 100 examinees. You have already learned by now how to generate synthetic data in Python. Now we generate the scores randomly and visualize using a bar plot hoping that no one scores below 50.

Here is how you do it in Python:

<script src="https://gist.github.com/albertyumol/95649098fa8c08dd58ce5e966b57b886.js"></script>

And here is the resulting distribution:

{:refdef: style="text-align: center;"}
<img src="{{ site.url }}{{ site.baseurl }}/images/stat/eskwelabs_score.png" alt="Eskwelabs scores." class="center">
{: refdef}

Visualizing can give us a feel of what the data looks like. We can see that one student actually perfected the exam (**maximum**) and the lowest score is around 25 (**minimum**). From these we can tell that the scores **ranges** from 25-50.

Most of the time, visualizing data is not enough to get insights. That is where statistics comes in.

Statistics refers to the mathematical methods and techniques to gain insights from data. I know you already did some statistics before. So this should be easy breezy.

The three most common tool used in data description is what we call the measures of central tendency.

<h1>Measures of Central Tendencies</h1>

In exploring our data sets, we want to check the average or central values of our data. Centrality can be measured by these 3 values: mean, median, and mode.

**Mean** <br>

The mean gives you an initial summary of your data set. It is obtained by getting the sum of the individual data points $$x_{i}$$ and divide this sum by the total number of data points $$N$$.

Mathematically:

$$
\begin{equation}
\overline x = \sum_{i}^{N} \frac {x_i}{N}
\end{equation}
$$

Programmatically in Python:

<script src="https://gist.github.com/albertyumol/a532da3d311d913247111c2485096231.js"></script>

Calculation the mean exam scores of Eskwelabs applicants, we find that the average score is $$36.9$$ which is $$73.8%$$. Not bad if the exam is difficult. This value somewhat gives us an idea of how the examinees performed overall in the exam.

Another measure of central tendency is the median.

**Median** <br>

The median is the middle value of the data set when you sort out every single points from lowest to highest. The most common approach in obtaining the median value is by considering two cases: when the length of data set $$N$$ is odd or even.

When $$N$$ is odd, it is not divisible by two thus we obtain a unique middle value.

Mathematically:

$$
\begin{equation}
M_{odd} = \left( \frac {N+1}{N} \right) ^{th} term
\end{equation}
$$

The other case is when $$N$$ is even which implies that there are two candidate values for the median. What you do is get the average of these two points.

Mathematically:

$$
\begin{equation}
M_{even} = \frac {\left( \frac {N}{2} \right) ^{th} term + \left( \frac {N}{2} + 1 \right) ^{th} term}{2}
\end{equation}
$$

Implementing both this equations in Python:

<script src="https://gist.github.com/albertyumol/15011622fbd3747ae541f45b5814b002.js"></script>

If you are keen enough to notice, in our mathematical equation for the even case we add 1 to the middle value but when we implemented in code we change it by subtracting one. This is because in Python, we start counting from 0 so we need to account for this translation.

The median score for the Eskwelabs exam data set is 37 which is not far from the mean. As we have observed, the median is a bit more complex to calculate because you need to sort first and find the middle value. There are other approaches for median calculation without the tedious sorting. See this link for exploration: [Sorting Less Median Calculation](https://medium.com/@nxtchg/calculating-median-without-sorting-eaa639cedb9f).

Although the mean is easier and faster to calculate, it is very sensitive to outliers unlike the median. So if your analysis requires less sensitivity to noise, median is the better descriptor.

As a side note, the median is a special type of measurement called **quantiles**. These refers to the partitioning of the data set in equal proportions. The median is actually a second order quantile because it divides the data set into two. Here is the general code to calculate the quantile based on order (or number of partitions):

<script src="https://gist.github.com/albertyumol/cb1434ebab9db42a6d4cdab472d2f43e.js"></script>

The last measure of central tendency is the mode.

**Mode** <br>

The mode is a measure of frequency. It gives you the most frequent value in your data set. You can use this to check the balance of your data set if it is biased for particular values. It can have more than one value if there is a tie for the maximum number of counts. The mode requires no formula since it is just the most frequent data point. One cool approximation for the mode by Professor Karl Pearson is given by the empirical formula:

$$
Mode = 3(Median) - 2(Mean)
$$

<script src="https://gist.github.com/albertyumol/2d58c12acc98fcc7c5242c05377805c8.js"></script>

<h1>Measures of Dispersion</h1>

Dispersion is the measure of the spread. It tells you the difference in values across all data points. A small dispersion value indicates the data points are near each other while a large dispersion means data points are far apart.

Basic dispersion measures include the range, variance, standard deviation, and interquartile range.

The range is self explanatory. It provides you the upper and lower bound of your data set. Mathematically:

$$
Range = x_{max} - x_{min}
$$

In Python,

<script src="https://gist.github.com/albertyumol/dc373d15b5c8f4105e7b6a4b411c08f3.js"></script>

The problem with the range is that it is only describing the end points of the data set. It doesn't provides us insights on the whole data set between in. To address this, we consider the next metric called variance.

Variance measure how far the data points are spread from the mean. Mathematically:

$$
\sigma ^{2} = \sum \frac {x_i - \overline x}{N}
$$

where $$x_i$$ is a single data point, $$\overline x$$ is the mean, and $$N$$ the total number of data points.

To do it in Python,

<script src="https://gist.github.com/albertyumol/97e6a6db5b2306aa5e56494a5bd2ee9f.js"></script>

The variance will help us determine the size of the data spread but the problem is the units. The units for the measures of central tendencies and range are all the same but variance is in squared units. It will make more sense if we get the square root variance (same unit with the other measures) and we call it the standard deviation given by

$$
\sigma = \sqrt {\sigma ^{2}} = \sqrt {\sum \frac {x_i - \overline x}{N}}
$$

In Python,

<script src="https://gist.github.com/albertyumol/88ea806ad551a42562a80bb159db6ce2.js"></script>

The standard deviation measures the absolute variability of the dispersion with respect to the mean. However, like the range it is also very susceptible to outliers. Depending on applications, a better metric would be the interquartile range. This can be calculated from the difference of $$75^{th}$$ and $$25^{th}$$ percentile value. In python,

<script src="https://gist.github.com/albertyumol/775f04f1fc5fc71b0d8009bbffd0396f.js"></script>

<h1>Covariance, Correlation, and Causation</h1>

Statistics has also some philosophical roots. We often hear the phrase, *'correlation is not causation'*. Use correlation with a grain of salt. You may be mislead by simplified assumptions and confounding variables.

For example, if there exists a strong correlation between variables x and y, we conclude:

(1) x causes y
(2) y causes x
(3) both causes each other
(4) neither causes each other

Recall that the variance is a measure of how a single variable deviates from the mean, a **covariance** measures the variation of two variables with respect to each other and their means. Mathematically:

$$
\sigma_{xy} = \frac {1}{N} \sum_{i} (x_i - \overline x)(y_i - \overline y)
$$

In Python,



The multiplication factor from the equation above is a dot product of two variables. A significantly positive covariance indicates that x is large when y is large and vice versa. A significantly negative covariance indicates that x is large when y is small and vice versa. A zero covariance indicates that there is no relation between the two variables.

We are now faced with the problem of interpreting the variance. Typically it is difficult to set how big a number is to be significant relative to other values. Another problem is the units, in this example x-units-y-units-per-unit-time. That is why we transform the covariance by normalizing it with the standard deviations of both variables $$x$$ and $$y$$. We call this transformed metric the correlation $$\rho_{xy}$$ mathematically as:

$$
\rho_{xy} = \frac {\sigma_{xy}}{\sigma_{x} \sigma_{y}}
$$

In Python,



This value is unitless (since we normalized) and ranges from [-1, 1]. We interpret values as:

(1) 1   - perfect correlation

(2) 0   - no correlation

(3) -1  - perfect anti-correlation






Exercises:

(1) Generate two random samples: x ~ N(1.78,0.1) and y ~ N(1.66,0.1).

(2) Compute $$\overline x$$, $$\sigma_{x}$$, $$\sigma_{y}$$, $$\sigma_{xy}$$, $$\rho_{xy}$$ from scratch (without using any module e.g. numpy, statsmodels). Compare the results with the ones from the numpy library and state your observations.


For further reading:

+ SciPy
+ pandas
+ StatsModels
+ OpenIntro Statistics
+ OpenStax Introductory Statistics











SCRATCH:

This is a guide on Markdown [Stat][1].


Here are my resources:

**References**

[1]: http://en.wikipedia.org/wiki/Markdown        "Stat"

[1] Joel Grus. Data Science from Scratch: First Principles with Python.

[2] something. something. Retrieved from:
[something](https://en.wikipedia.org/wiki/Global_Database_of_Events,_Language,_and_Tone)

[3] something. Retrieved from: [something](https://www.gdeltproject.org/)

[4] something. something. Retrieved from: [something](https://en.wikipedia.org/wiki/Conflict_and_Mediation_Event_Observations)

[5] something. something. Retrieved from: [something](https://www.hindawi.com/journals/ddns/2017/8180272/)

[6] something. something. Retrieved from: [https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-019-0183-y](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-019-0183-y)

[7] something. something. Retrieved from :
[something](https://tenor.com/view/well-be-watching-you-greta-thunberg-gif-15167876)


<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6410209740119334",
    enable_page_level_ads: true
  });
</script>

<div class="fb-comments" data-href="https://albertyumol.github.io/" data-numposts="5"></div>
